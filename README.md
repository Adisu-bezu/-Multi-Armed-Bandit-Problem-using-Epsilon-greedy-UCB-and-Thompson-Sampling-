# -Multi-Armed-Bandit-Problem-using-Epsilon-greedy-UCB-and-Thompson-Sampling-
This repository implements Multi-Armed Bandit algorithms: Epsilon-Greedy, UCB, and Thompson Sampling. It includes experiments analyzing their performance and trade-offs. 
